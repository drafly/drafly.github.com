---
layout: post
title: "机器学习－逻辑回归与最大似然估计"
description: ""
category: "机器学习"
tags: [机器学习, 凸优化]
---
{% include JB/setup %}

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>

在(梯度下降算法)[http://www.hanlongfei.com/凸优化/2015/07/29/gradient/]一文中，我们提到了线性回归和其对应的几种优化方法，本文主要介绍逻辑回归（*Logistic Regression*）和最大似然估计。线性回归和逻辑回归是机器学习算法中最常用的两个算法，分别用于预测和分类，属于有监督学习。在有监督学习中，如果因变量为有限个离散值，则预测问题就转变成分类问题，从这个角度也可以说分类问题是预测问题的一个特例。此时，通过样本集训练获得的有监督学习模型，即逻辑回归模型就称为分类模型或分类决策函数，也称为分类器（*Classifier*）。

##1. 逻辑回归
对于某个分类任务，如判定一封邮件是否为垃圾邮件，我们需要通过分类器预测分类结果：是（记为1）or不是（记为0）。如果我们考虑0就是“不发生”，1就是“发生”，那么我们可以将分类任务理解成估计事件发生的概率$$p$$，通过事件发生概率的大小来达到分类目的。因此，我们需要使得预测结果即概率值限定在0～1之间，很明显，如果我们仍然采用线性回归$$p=f_{\theta}(x)=\theta^T x$$作为分类器，其预测值可能会远远大于1或者远远小于0，不符合我们预期想法。所以，逻辑回归引入*logistic*函数，将分类器输出界定在［0，1］之间，其一般形式可表示为$$f_{\theta}(x)=g(\theta^T x)$$。那么问题来了，我们到底需要选择什么样形式的*logistic*函数$$g(z)$$？

需要强调的是，我们不选用线性回归的原因是线性回归不能保证预测值的范围位于［0，1］之间。所以，针对该问题最简单的方法是移除因变量值域的限制，这样，我们就需要对概率形式做某种变换。

首先，选用优势比*Odds*代替概率，优势比是事件发生概率和不发生概率之间的比值，记为$$odds = \frac{p}{1-p}$$，通过该变换，我们可以将［0，1］之间的任意数映射到$$[0,\infty]$$之间的任意实数。但是，线性回归的输出还可以是负数，我们还需要另一步变换将$$[0,\infty]$$的实数域映射到这个实数域$$\mathbb{R}$$空间；

然后，在众多非线性函数中，*log*函数的值域为整个实数域且单调，因此，我们可以计算优势比的对数，另$$\eta = log(odds)=log \frac{p}{1-p}=logit(p)$$。

经过以上两步，我们可以去除分类问题对因变量值域的限制，如果概率等于0，那么优势比则为0，*logit*值为$$-\infty$$；相反，如果概率等于1，优势比为$$\infty$$，*logit*值为$$\infty$$。因此，*logit*函数可以将范围为［0，1］的概率值映射到整个实域空间，当概率值小于0.5时，*logit*值为负数，反之，*logit*值为正数。

综上所述，我们解决了分类问题对分类器预测的因变量值域的限制，我们就可以采用线性回归对一一映射后的概率值进行线性拟合，即$$logit(p)=log(\frac{p}{1-p})=\eta=f_{\theta}(x)=\theta^T \cdot x$$。因为logit变换是一一映射，所以存在logit的反变换（antilogit），我们可以求得$$p$$值的解析表达式：

$$p=antilogit(x)=logit^{\eta}=\frac{e^{\eta}}{1+e^{\eta}}=\frac{e^{\theta^T \cdot x}}{1+e^{\theta^T \cdot x}}$$

$$1-p=1-antilogit(x)=1-logit^{\eta}=1-\frac{e^{\eta}}{1+e^{\eta}}=1-\frac{e^{\theta^T \cdot x}}{1+e^{\theta^T \cdot x}}＝\frac{1}{1+e^{\theta^T \cdot x}}$$

众所周知，上式即为logistic回归的一般表达式，其采用的logit变换一般记为*sigmoid*函数：

$$g(x)=\frac{e^{\theta^T x}}{1+e^{\theta^T x}}$$

讲到这里，还有一个问题是为什么要选用sigmoid函数呢？为什么不选用其他函数，如probit函数？

其实，无论是sigmoid函数还是probit函数都是广义线性模型的连接函数（*link function*）中的一种。选用联接函数是因为，从统计学角度而言，普通线性回归模型是基于响应变量和误差项均服从正态分布的假设，且误差项具有零均值，同方差的特性。但是，例如分类任务（判断肿瘤是否为良性、判断邮件是否为垃圾邮件），其响应变量一般不服从于正态分布，其服从于二项分布，所以选用普通线性回归模型来拟合是不准确的，因为不符合假设，所以，我们需要选用广义线性模型来拟合数据，通过标准联接函数(canonical link or standard link function)来映射响应变量，如：正态分布对应于恒等式，泊松分布对应于自然对数函数，二项分布对应于logit函数（二项分布是特殊的泊松分布）。因此，说了这么多是想表达联接函数的选取除了必须适应于具体的研究案例，不用纠结于为什么现有的logistic回归会选用sigmoid函数，而不选用probit函数，虽然网上也有不少说法说明为什么选择sigmoid函数，例如“该函数有个漂亮的S型”，“在远离x＝0的地方函数的值会很快接近0/1”，“函数在定义域内可微可导”，这些说法未免有些“马后炮”的感觉，哪个说法仔细分析都不能站住脚，我觉得选用sigmoid函数也就是因为该函数满足分类任务，用的人多了也就成了默认说法，这跟给物体取名字有点类似的感觉，都有些主观因素在其中。

##2. 代价函数
与线性回归一样，我们同样可以采用均方误差函数作为逻辑回归的代价函数，则其一般形式可以表示为：

$$J(\theta_0,\theta_1,\ldots,\theta_n)=\sum_{i=1}^{m}(g(\theta^T \cdot x^{(i)}) - y^{(i)})^2$$

但是，该函数不是凸函数，在之前介绍的凸优化问题中我们也提到，非凸函数不易求的最值，而凸函数具有“局部最小值点即为全局最小值点”的特性，所以，我们一般会将非凸函数转换为等价的凸函数进行求解。

##3. 最大似然估计