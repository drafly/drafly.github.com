---
layout: post
title: "CMU10725－凸优化－INtro"
description: ""
category: "凸优化"
tags: [凸优化, CMU10725]
---
{% include JB/setup %}

题外话，来到CMU已经两周了，从刚开始的难以开口到现在可以沟通，从四处借东西蹭吃喝到现在学习做饭健身三不误，渐渐的开始适应了美国气息的生活。总的感觉，在这里无论是国人还是老外，自主性都很强，每天都有为之努力的目标，不知道这是不是跟due或者所谓的压力有关。这学期旁听了三门课，接下来要写的《凸优化》系列学习笔记就是其中之一，另两门是“16720－Computer Vision”和“10715－Advanced Introduction of Machine Learning”。在日后的时间里，我也会把机器学习相关的算法继续补充完整。

整体来说，从听课和做作业的角度来讲，CMU课程的强度之大是公认的，一学期最多修3-4门课就不错了，不像国内的课程，很多硕士一学期会选9-10门课程，然并卵，走形式而已，真正可以学到的知识少之又少。这一现象基本上是国内各大院校的普遍现象，如果想要提高国内科研水平，提高课程质量和效果是必须的。因为，CMU本科生或者硕士生一年课程代码量基本在5万行，而且对于领域相关知识的掌握很充分，相比之下，国内本科生基本不写代码，硕士毕业下来代码量也不会超过2万行，加上上课基本没什么用，所以理论与实践上的差距就体现出来了。当然，开篇废话这么多，无外乎是想把自己感受到的表达出来，生活是需要积极向上的，不能沉醉于平稳和安逸，每来到一个新的环境，总发现自己low到极点，算是学渣了吧，希望离开的时候起码可以称为合格的一员。

好了，废话不多说，开始整理凸优化课程的学习笔记～～

### 1. Tibshirani
感谢Ryan Tibshirani老师为这门课开源做出的贡献，希望更多人可以更好的了解凸优化，明白其在机器学习研究领域的重要性。所有课件和视频都可以在[课程官网](http://www.stat.cmu.edu/~ryantibs/convexopt/)上找到。

### 2. 优化理论
对于优化理论的定义，Ryan给出了一副很好理解优化理论是做什么的图，优化理论就是用于求解某个函数的极值，至于极值体现的含义就跟你要解决的问题相关了，与机器学习的“没有天生优越的分类器”的理论类似，优化算法也没有好坏之分，只有适合还是不适合解决某一问题。

<div align="center">
<img src="/img/R/cmu10725/whatisop.jpg" width="400" align="middle"/>
</div>

### 3. 凸集和凸锥
#### a. 凸集
仿射集和凸集的概念在[凸优化－凸集](http://www.hanlongfei.com/凸优化/2015/05/22/convexset/)一文中做了详细介绍。

凸集为 $$C \subseteq \mathbb{R}^n$$，使得$$x,y \in C \longrightarrow tx+(1-t)y \in C$$，其中$$0 \leq t \leq 1$$。具体表现为下图，左边第一个为凸集，第二个则不是凸集。

<div align="center">
<img src="/img/R/cmu10725/convexset.jpg" width="400"/>
</div>

任何凸集的线性组合仍为凸集，所有凸集的集合为*Convex hull*。空集、点、线、球体（Norm ball:$${x: \parallel x \parallel \leq r}$$）、超平面（Hyperplane:$${x: a^Tx=b}$$）、半空间（Halfspace:$${x: a^Tx \leq b}$$）、仿射空间（Affine space:$${x: Ax=b}$$）和多面体（Polyhedron$${x: Ax \leq b}$$）均为凸集。

<div align="center">
<img src="/img/R/cmu10725/polyhedron.jpg" width="400"/>
</div>

下面证明为什么Norm ball是凸集：

对于Norm ball中的任意两点$$x和y$$，可以根据三角不等式获得：

$$\parallel tx + (1-t)y \parallel leq t \parallel x \parallel + (1-t) \parallel y \parallel \leq tr + (1-t)r \leq r$$

所以，Norm ball中的任意两点$$x和y$$的线性组合仍在集合内，证明Norm ball为凸集。

其中，Ryan还提到一个问题就是关于多面体，集合$${x: Ax \leq b, Cx=d}$$是否仍是Polyhedron？

结果是显然的，仍然是多面体，因为平面$$Cx=d$$可以写成$$Cx \leq d和-Cx \leq -d$$，满足Polyhedron的定义，即$${x: Ax \leq b}$$。

#### b. 凸锥
锥（cone）是指$$x \in C \rightlongarrow tx \in C$$，其中$$t \geq 0$$，而凸锥（convex cone）的定义则是$$x_1,x_2 \in C \rightlongarrow t_1x_1 + t_2x_2 \in C$$，其中$$t_1,t_2 \geq 0$$，凸锥的实例如下图：

<div align="center">
<img src="/img/R/cmu10725/convexcone.jpg" width="400"/>
</div>

但是，是否可以举一个例子，使得集合$$C$$为锥，但不是凸锥呢？如下图，如果两条直线组成的锥，分别从直线上选两点，则亮点的线性组合不一定在这两条直线上。

<div align="center">
<img src="/img/R/cmu10725/cone.jpg" width="400"/>
</div>

对于凸锥，其中两个重要的实例为Norm cone和Normal cone，接下来着重介绍这两个凸锥的概念。

Norm cone是指$${(x,t):\parallel x \parallel \leq t}$$，而second-order cone则是使用2阶范数$$norm \parallel \cdot \parallel_2$$，如果只看定义的话不太好理解Norm cone是什么，下面给出了matlab仿真得到的三维下的Norm Cone实例：

<div align="center">
<img src="/img/R/cmu10725/normcone.jpg" width="400"/>
</div>

Normal cone是指给定任意集合$$C$$和点$$x \in C$$，$$\mathbb{N}_C(x)={g:g^Tx \geq g^Ty}$$，其中$$y \in C$$。无论集合C是否是凸集，满足该定义的点的集合都是Normal cone，其含义是指Normal cone中的点与集合C内的点$$x$$的内积永远大于集合C内任意点与其点x的内积。具体实例如下图，$$x'$$normal cone是集合C边缘切线向量之间的点的集合：

<div align="center">
<img src="/img/R/cmu10725/normalcone.jpg" width="400"/>
</div>

normal cone的定义将会在subgradient descent算法中应用到。

### 4. 凸函数
凸函数为函数$$f$$，$$f: \mathbb{R} \rightarrow \mathbb{R}$$，使得函数$$f$$的定义域为凸集，$$dom(f) \subseteq \mathbb{R}^n$$，且$$f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)$$。其中$$0 \leq t \leq 1$$。具体表现为下图：

<div align="center">
<img src="/img/R/cmu10725/convexfun.jpg" width="400"/>
</div>

### 4. 凸优化问题
优化问题的定义为：

\begin{aligned}
min_{x \in D} f(x) \\\
subject \; to \quad g_i(x) \leq 0, \quad i=1, \ldots, m \\\
h_j(x)=0, \quad j=1, \ldots, r
\end{aligned}


其中，$$D=dom(f) \cap \bigcap_{i=1}^m dom(g_i) \cap \bigcap_{j=1}^pdom(h_j)$$。

而凸优化问题相对优化问题的定义而言，要求函数$$f$$和$$g_i(x)$$是凸函数，$$h_j(x)=a_j^Tx+b_j$$是仿射函数。

对于仿射集合凸集的差别在[凸优化－凸集](http://www.hanlongfei.com/凸优化/2015/05/22/convexset/)一文也做了分析，二者的差别就在于凸集是线段而仿射集是直线（一维情况下）。

对于凸优化问题，凸函数有一个特别的性质，即局部最优解是全剧最优解，即如果$$x \in D$$，同时$$x$$满足所有约束，那么对于局部$$y,\parallel x-y \parallel_2 \leq \rho$$，当$$f(x) \leq f(y)$$时，对于所有可行解$$y, f(x) \leq f(y)$$。这就意味着在优化求解过程中可以省略很多麻烦～

<img src="/img/R/cmu10725/localminima.jpg" width="400"/>