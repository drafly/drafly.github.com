---
layout: post
title: "凸优化－梯度下降算法"
description: ""
category: "凸优化"
tags: [机器学习, 凸优化]
---
{% include JB/setup %}

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>

## 1. 线性回归
从机器学习角度而言，线性回归也属于有监督学习方法，即数据使用类似类似$$a_i^T x - b_i$$的线性预测函数来建模，并且未知的模型参数即线性函数自变量的系数通过数据来估计。在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。线性回归认为自变量和因变量之间是线性关系，所以拟合的模型是一条广义的直线。线性回归的代价函数可以理解为争取让所有的样本点都落在直线上，即离直线的距离很近，线性回归的代价函数可以写为：

$$minimize\ f_0(x)=\parallel Ax-b \parallel^2=\sum_{i=1}^{k}(a_i^T x - b_i)^2 \quad$$ （1.1）

利用最小二乘法我们可以获得该代价函数的解析表达式：

$$x=(A_T A)^{-1}A_T b$$   (1.2)

## 2. 凸优化
在[凸优化——基本概念](http://www.hanlongfei.com/optimization/2015/05/20/convexoptimization/)一文中，我已经对最优化和凸优化的概念做了介绍，也介绍了最小二乘问题（即目标函数是具有线性表达式平方和的形式）。对于类似(1.1)式$$(a_i^T x - b_i)^2$$的优化函数，很明显是凸优化函数，但是当样本量和特征纬度过高时，最小二乘法的计算时间会成指数增长，因此，我们一般会选择梯度下降的算法对其进行求解。

## 3. 梯度下降原理
[梯度下降方法](https://zh.wikipedia.org/wiki/梯度下降法)在wiki上的解释为如果目标函数$$F(x)$$在点$$a$$处可微且有定义，那么函数$$F(x)$$在点$$a$$沿着梯度相反的方向$$-\nabla F(a)$$下降最快。其中，$$\nabla$$为梯度算子，$$\nabla = (\frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_2}, \ldots, \frac{\partial}{\partial x_n})^T$$。那么，为什么要使得下降方向为梯度的反方向呢？接下来就对该理论作简要证明。

我们之前学习过，一维函数在某一点的导数描述了这个函数在这一点附近的变化率。导数的本质是通过极限的概念对函数进行局部的线性逼近。假设函数$$y=f(x)$$在点$$x_0$$的某个邻域内有定义，当自变量x在点$$x_0$$处取得增量$$\vartriangle x$$时，相应地函数y取得增量$$\vartriangle y = f(x_0 + \vartriangle x) - f(x_0)$$，如果当$$\vartriangle x \rightarrow 0$$时，$$\vartriangle y$$与$$\vartriangle x$$之比的极限存在，则称函数在该点可导，该极限也表示函数因变量在该点的变化率。因此，导数的概念就是函数变化率这一概念的精确描述。

对于多元函数，函数在某点的偏导数则对应的反应了函数值沿该坐标轴方向的变化率。对于可微函数而言，不仅有关于各个自变量的偏导数，而且有沿任何方向的方向导数。方向导数较之偏导数而言，是偏导数概念的推广，指多元函数在某点沿该向量方向变动的瞬时变化率。因为，方向导数代表函数在某点沿方向向量方向变动的大小，所以对于（1.1）式而言，我们**要找到目标函数在某点变化最快的方向，从而能更快的达到目标函数的极小值**。而如果一个标量场在某点沿任意方向的方向导数都存在，则其中必有最大的一个。因此，我们只要**证明出方向导数的极值等于其梯度的范数**时，即可证明出梯度下降算法的下降方向为多元函数在该点的梯度。

设$$f$$是定义于$$\mathbb{R}^n$$中某区域D上的函数，点$$p_0 \in D$$，点p为一动点，如果极限$$\lim\limits_{\parallel p_0 \cdot p \parallel \rightarrow 0} \frac{f(p)-f(p_0)}{\parallel \vec{p_0 \cdot p} \parallel}$$存在，则称此极限为函数f在点$$p_0$$处沿$$\vec{p_0 \cdot p}$$方向上的方向导数。对于可微函数在某点可导而言，则在某点必然存在关于各个自变量的偏导数，而且有沿任何方向的方向导数，同时，**这些方向导数还可以用偏导数来表示**。

为用偏导数表征方向导数，首先要介绍方向余弦的概念，设$$\ell$$是n维非零向量，$$\ell_0=\frac{\ell}{\parallel \ell \parallel}$$为与$$\ell$$方向相同的单位方向向量，若取$$0 \leqslant \alpha_i \leqslant \pi$$，使得$$\ell_0 = (cos\alpha_1, \ldots, cos\alpha_n)$$，其中，$$cos^2\alpha_1 + \cdots + cos^2\alpha_n=1$$，则称$$(cos\alpha_1, \ldots, cos\alpha_n)$$为向量$$\ell$$的方向余弦。

所以，如果函数$$f$$在点$$p_0=(x_1, \ldots, x_n)$$处可微，向量$$\vec{p_0 \cdots p}=(\vartriangle x_1, \ldots, \vartriangle x_n)$$与$$\ell$$同向，则函数在该点沿$$\ell$$方向的方向导数$$\frac{\partial f}{\partial \ell}\lvert_{p_0}=\lim\limits_{\parallel \ell \parallel \rightarrow 0} \frac{f(p_0+\parallel \ell \parallel \ell_0)-f(p_0)}{\parallel \ell \parallel}=\lim\limits_{\parallel p_0 \cdot p \parallel \rightarrow 0} \frac{f(p)-f(p_0)}{\parallel \vec{p_0 \cdot p} \parallel}=\lim\limits_{\parallel p_0 \cdot p \parallel \rightarrow 0} \frac{f(p_0+\vec{p_0 \cdot p})-f(p_0)}{\parallel \vec{p_0 \cdot p} \parallel}$$;

由于函数$$f$$可微，则函数$$f(p)-f(p_0)$$可以增量表示$$f(p_0+\vec{p_0 \cdot p})-f(p_0)=f(p_0 + \vartriangle x) - f(p_0)=f(x_1 + \vartriangle x_1, \ldots, x_n + \vartriangle x_n) - f(x_1, \ldots, x_n)=\frac{\partial f}{\partial x_1}\lvert_{p_0} \vartriangle x_1 + \cdots + \frac{\partial f}{\partial x_n}\lvert_{p_0} \vartriangle x_n + o(\parallel \vec{p_0 p} \parallel)$$;

因此，$$\lim\limits_{\parallel p_0 \cdot p \parallel \rightarrow 0} \frac{f(p)-f(p_0)}{\parallel \vec{p_0 \cdot p} \parallel}=\lim\limits_{\parallel p_0 \cdot p \parallel \rightarrow 0} \lbrack \frac{\partial f}{\partial x_1}\lvert_{p_0} \frac{\vartriangle x_1}{\parallel \vec{p_0 \cdot p} \parallel} + \cdots + \frac{\partial f}{\partial x_n}\lvert_{p_0} \frac{\vartriangle x_n}{\parallel \vec{p_0 \cdot p} \parallel} + \frac{o(\parallel \vec{p_0 \cdot p} \parallel)}{\parallel \vec{p_0 \cdot p} \parallel} \rbrack=\frac{\partial f}{\partial x_1}\lvert_{p_0} cos\alpha_1 + \cdots + \frac{\partial f}{\partial x_n}\lvert_{p_0} cos\alpha_n$$；

上式又可以转变为$$\nabla f \cdot \ell_0=\parallel \nabla f \parallel_2 \cdot \parallel \ell_0 \parallel_2 \cdot cos(\nabla f, \ell_0)=\parallel \nabla f \parallel_2 \cdot cos(\nabla f, \ell_0)$$。

综上所述，函数$$f$$在点$$p_0$$的方向导数$$\frac{\partial f}{\partial \ell}\lvert_{p_0}=\parallel \nabla f \parallel_2 \cdot cos(\nabla f, \ell_0)$$，推导完成后，可能看到这又忘记我们要证明什么了。。。我们的目标是找到方向导数中的极值，因此，当且仅当$$cos(\nabla f, \ell_0)=1或－1$$时，可取得极值，**考虑到我们是要使得目标函数最小，所以这里应该选择cos值为－1，这样目标函数会沿该方向下降最快**；同理，若要使得目标函数上升最快，则应选择cos值为＋1，这样目标函数会沿该方向上升最快。而cos值等于－1等价于向量$$\nabla f$$与向量$$\ell_0$$平行且方向相反，即方向导数$$\ell_0$$为梯度的负方向，此时，函数$$f$$下降最快。即证**多元函数沿其负梯度方向下降最快**。